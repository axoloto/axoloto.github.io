---
layout: post
title: gpu fluids simulator
description: >
  Implementation of a Position Based Fluids real-time simulator
  
  **Source code available [here](https://github.com/axoloto/RealTimeParticles).**
sitemap: false
hide_last_modified: false
category : rtp
image: /assets/img/blog/fluids/fluids-bg.jpg
accent_image: 
  background: url('/assets/img/blog/fluids/fluids-bg.jpg') center/cover
  overlay: false
accent_color: '#ccc'
theme_color: '#ccc'
---

# Position Based Fluids

1. this list will be replaced by the table of contents
{:toc}

## Introduction

To acquire knowledge and expertise in a domain, I tend to mostly rely on two mechanisms:
1. **Aiming high** (sometimes stupidly high)
2. **Splitting complex challenges in smaller ones to tackle them separately**

Following and combining those two basic principles usually works pretty well for me. I set high goals far ahead, partition the path leading to it in a multitude of intermediary steps, and patiently reach everyone of them to finally arrive at the ultimate objective.

Hence the boids behaviour model described in my latest [post](https://axoloto.github.io/rtp/2021-07-11-boids/) was just a step. I greatly enjoyed it but it was first of all a necessary intermediary stage. I had something more complicated in mind, a physics model like fluids or combustion, but I wanted to separate theorical and technical challenges.

Boids project was all about setting up a C++ project from scratch, creating an OpenGL render, an OpenCL wrapper and connecting them altogether. No significant theorical obstacles but a lot of technical difficulties to handle one by one. Real physics models usually come with complex ordinary or partial differential equations (ODE/PDE), adding another mathematical layer of complications. I wanted to have strong foundations before playing with it. 

Well guess what; boids simulation being fully functional, now is the time for the real game: **implementing a real-time 3D fluids simulation**! Let's begin.

## Simulating Fluids with Math and bits

**I always found amazing that such a complicated thing like fluids dynamics can be modelized by a couple of differential equations.** Fluids motions can be incredibly complex, both temporally and spatially, nevertheless some 19<sup>th</sup> century geniuses, namely Claude-Louis Navier and George Gabriel Stokes, brought up a pair of PDEs and showed that by approximately solving them you could get very close from many fluids behaviors present in our world. Crazy, right? I say approximately because the Navier-Stokes equations have no evident solutions in 3D and therefore are solved using numerical or analytical models based on different approximations. **So, before implementing anything, we need to decide which of those approaches suits the best our needs.**

By the way, if you have 3D solutions of Navier-Stokes equations written on your fridge, don't hesitate to share it to the world because it is among the [seven most wanted open problems in mathematics](http://www.claymath.org/millennium-problems/navier%E2%80%93stokes-equation) ($1 million prize).

$$
\begin{aligned}
\nabla \cdot \mathbf{u} &= 0 \\
\rho ( \frac{\partial \mathbf{u}}{\partial t} + \mathbf{u} \cdot \nabla \mathbf{u} ) &= - \nabla p + \rho \mathbf{g} + \mu \nabla^2 \mathbf{u}
\end{aligned}
$$

*Miss Equation 1822, the Navier-Stokes equations (Eulerian description for a viscous incompressible fluid)*

### Eulerian or Lagrangian? To be or not to be?

Before diving into numerical schemes and solvers, first we need to choose between two different ways of looking at fluids motion:
1. **Eulerian description** - **We look at the fluids domain globally, considering it as a vector field.** The fluids behavior is analyzed and solved through time at specific fixed places in the studied domain. This approach is extensively used in the **computer fluids dynamics** (CFD) field where scientists and engineers apply it to analyze and solve real world fluids problems, for example reducing aircraft drag or improving wind turbines efficiency. **This approach can gives very precise estimations of flow motions and forces.** I used it in my master thesis as most of the Fluids Lab I was working at. Unfortunately **it can be extremely costly**, some 3D heavy simulations can literally take weeks or months to proceed. Besides, particles movements can be obtained by post-processing the fluids velocity field but they are not directly considered in the solver.

2. **Lagrangian description** - **We look at the fluids as a group of individual particles, analyzing their interactions to determine their trajectories and conditions.** Contrary to the Eulerian approach, the domain is not considered as a whole and its importance is less significant. Hence, no need of mesh or grid, simply a bunch of particles on which we apply some rules modelizing the fluids interaction. To make it simple, **it is a bit like approximating a fluids domain with a ball pit**. Historically, Lagrangian simulations have been considered less precise than Eulerian ones. One of their biggest forces is their execution speed, they **can be used for coarse but fast and visually acceptable simulations**. Thus, they are particularly apprecieted in the world of **computer graphics**. By being meshfree, it is also well suited for free-surface problems or ones with large boundary displacements.

In short, by using Lagrangian description we can favor interactivity over accuracy. **Hence, a Lagrangian solver clearly sounds as the right candidate for our real-time particle simulator.** We "simply" need to apply the right combinations of rules on each particles to give them a realistic visual sense of fluids motion. This is the topic of the next part.

### Lagrangian computational methods

#### The old venerable master

**Smooth Particles Hydrodynamics (SPH)** is one of the major computational methods used in Computer Graphics for fluids. Initially developped by Gingold, Monaghan and Lucy in 1977 for astronomical problems (article), since then **it has been extensively used in fluids simulations, for its simplicity of implementation and execution**. Combined with some post-processing rendering, it can deliver very realist free-surface interactive simulations.

**SPH method divides the fluid continuum in a set of discrete particles.** For each particle, SPH interpolates its fluids properties using its neighbors and functions quantifying the influence of each of them. Called **smoothing kernel functions**, those functions are a central component of the method. They determine how particles interact with each others and must be carefuly designed. Using this method, we can **compute pressure and viscosity forces on each particle and deduce its acceleration through Newton's second law.** Then, its integration gives us the velocity and finally the position of the particle. To facilitate nearest neighbor search (NNS), SPH method can also use some spatial partioning. Particles, sum of neighbors contributions, spatial partioning... sounds familiar, right? Don't be surprised, **it is strongly similar to the existing implementation used for the boids model and that's very good news.**

\begin{equation}
 A(\mathbf{x}) = \sum_j m_j \frac{A_j}{\rho_j} W(\mathbf{x} - \mathbf{x_j}, h)\end{equation}

*Interpolation of a scalar quantity A at $$\mathbf{x}$$ by a weighted sum of contributions from its neighbor particles $$j$$*

$$
\begin{aligned}
W_{poly6}(\mathbf{r}, h) = \frac{315}{64 \pi h^9} 
      & \left\{
    \begin{array}{lr}
      (h^2 -\Vert r \Vert^2)^3 & 0 \leq r \leq h\\
      0 & otherwise
    \end{array}
    \right.    \\
W_{spiky}(\mathbf{r}, h) = \frac{15}{\pi h^6} & \left\{
     \begin{array}{lr}
      (h - \Vert r \Vert)^3 & 0 \leq r \leq h\\
      0 & otherwise
    \end{array}
    \right. 
\end{aligned}
$$

*Exemple of smoothing kernel functions used in our model and mentioned in [Muller et al., 2001](https://matthias-research.github.io/pages/publications/sca03.pdf)*

Despite its charms, after reading a few papers on SPH, **I realized that it might not be the perfect solution.** **Traditional SPH model can rapidly shows instabilities when some particles have few or no neighbors, injecting negative pressure in the domain.** There is a vast number of variants made in the past decades to address those issues and improve the initial model, PCISPH, XSPH... Modern SPH results are much better, but the **use of small time steps for the simulation is still necessary, limiting simulation speed.** Hence, even if you manage to have a simulation running at 60 fps, the motion of particles might be visually slow.

#### A newcomer in town

While reading a well-known fluids SPH paper called [Particle-Based Fluid Simulation for Interactive Applications](https://matthias-research.github.io/pages/publications/sca03.pdf) written in 2001, one authors name caught my attention: Matthias Muller. He is currently the physics research lead at NVIDIA, and I remembered that I had seen another computational method on his personal page a while ago. 

Called **Position Based Dynamics** (PBD), this framework was presented in 2006 and has been used for fluids simulation in a paper named [**Position Based Fluids**](https://mmacklin.com/pbf_sig_preprint.pdf) (PBF). It was published in 2013 by Muller and Macklin, 12 years after the initial SPH paper. Personal rule of thumb; **when scientists wrote a paper on the same subject 10 years after the previous one, it is usually a good idea to read it**, they have probably something better to propose. This PBF paper brought indeed great insights to this new method but also highlighted the limitations of the SPH model. It also helped me understanding the differenciation between two keywords used in this field: **interactive vs real- time**. The first one means fast enough to play with interactively but potentially still visually too slow to be acceptable in many applications, the latter means interactive and fast enough for video games type application, exactly what we are looking for.

Many computation methods, SPH included, are based on Newton's second law. You compute the forces applied on each particle, use it to obtain its acceleration and from there integrate it twice to get its position. You repeat it for each time step and you have something moving. Position Based Dynamics/Fluids approach is radically different, you don't even compute the acceleration, you mostly focus on **particles position** and **constraints**. Constraints are designed functions defining how you want the system to behave, how you want the particles to be positioned between each other. You can imagine one constraint as a web connecting one particle to its neighbors and constraining them in order to have some physical property (energy, density or else) at its desired value. I find this approach very interesting. No double time integration and more explicit control on your system, as you manipulate directly particle positions. To improve precision and get closer to the wanted behavior, you can iterate on your constraint system at each time step. By doing so, **PBD brings more stability and allows us to increase simulation time steps, bringing us into the real-time world.**

### Position Based Fluids equations

#### Introduction to the concept of constraint
In their [article](https://mmacklin.com/pbf_sig_preprint.pdf), Macklin and Muller give a good overall presentation of their proposed framework. It is quite consequent and contains different specific ramifications and fine-tuning so I won't go into every details. Instead, **I want to focus on the key concept of constraint** because it took me a while to understand it completely and I think its comprehension is vital for any PBF implementation.

For each particle $$ p_i $$, Position Based Fluids approach sets a single positional constraint $$ C_i $$ on $$ p_i $$ itself and its neighbors, all their positions being contained in $$ \mathbf{X} = [\mathbf{x}_1, \ldots, \mathbf{x}_n]$$ with $$ i \in [1, \ldots, n] $$. The objective is to constraint each particle to reach a position $$ \mathbf{x}_i $$ where its density $$ \rho_i $$ tends towards the rest density $$ \rho_0 $$, a fluids-dependent property defined by the user.

$$ C_i(\mathbf{X}) = \frac{\rho_i}{\rho_0} - 1  \tag{1} $$

Interestingly, we rely on a SPH kernel function $$ W $$ to compute $$ \rho_i $$. In the article, they use the smoothing kernels $$ W_{poly6} $$ and $$ W_{spiky} $$ introduced previously.

$$ \rho_i = \sum_{j = 1}^n W_{poly6}(\mathbf{x}_i - \mathbf{x}_j, r) \tag{2} $$

For every constraint, we want to find the set of positions $$ \mathbf{X} $$ which forces the density $$ \rho_i $$ to be equal to $$ \rho_0 $$. Mathematically, it means that we want to find the particles positions correction $$ \Delta \mathbf{X}^i $$ giving us:

$$ C_i(\mathbf{X} + \Delta \mathbf{X}^i) = 0 \tag{3} $$

From there, we can try to minimize the constraint $$ C_i $$ by iterations with a gradient descent. To do so, we linearly approximate $$ (3) $$ and assume that $$ \Delta \mathbf{X}^i $$ is in the direction of the negative gradient of $$ C_i $$. Considering $$ \nabla C_i = \nabla_{\mathbf{X}} C_i = [\nabla_{\mathbf{x}_1} C_i, \ldots, \nabla_{\mathbf{x}_n} C_i]$$ :

$$ \Delta \mathbf{X}^i \approx \nabla C_i(\mathbf{X}) \lambda_i \tag{4} $$

$$
\begin{aligned}
C_i(\mathbf{X} + \Delta\mathbf{X}^i) & \approx C_i(\mathbf{X}) + \nabla C_i(\mathbf{X})^T  \Delta\mathbf{X} = 0  \\
      & \approx C_i(\mathbf{X}) + \nabla C_i(\mathbf{X})^T \nabla C_i(\mathbf{X}) \lambda_i = 0
\end{aligned} \tag{5}
$$

At this point, we can deduce the value of the density constraint coefficient $$ \lambda_i $$ :

$$ \lambda_i = - \frac{C_i(\mathbf{X})}{\sum_{j = 1}^n {\mid \nabla_{\mathbf{x}_j} C_i(\mathbf{X}) \mid}^2 } \tag{6} $$ 

Besides, using the SPH discrete definition of the density $$ (2) $$, we can define more precisely the gradients of $$ C_i $$ with respect to each particle:

$$
\nabla_{\mathbf{x}_k} C_i(\mathbf{X}) = \frac{1}{\rho_0} \left\{ 
    \begin{aligned}
     & \sum_{j = 1}^n \nabla_{\mathbf{x}_i} W_{spiky}(\mathbf{x}_i - \mathbf{x}_j, r), \space k = i \\ 
     & - \nabla_{\mathbf{x}_j} W_{spiky}(\mathbf{x}_i - \mathbf{x}_j, r), \space k = j
\end{aligned} \right. \tag{7}
$$

Remember that for each particle we set one constraint affecting this particle but also its neighbors. Hence for each particle, **its position is affected by its own constraint but also the ones set on its neighborhood.** Macklin and Muller superpose each contribution:

$$
\Delta \mathbf{x}_i = \sum_{j = 1}^n \Delta \mathbf{x}^j_i = \sum_{j = 1}^n  \lambda_j \nabla_{\mathbf{x}_i} C_j(\mathbf{X}) \tag{8}
$$

Finally, by connecting $$ (7) $$ and $$ (8) $$ and assuming $$ \nabla_{\mathbf{x}_i} W_{spiky}(\mathbf{x}_i - \mathbf{x}_j, r) = -\nabla_{\mathbf{x}_j} W_{spiky}(\mathbf{x}_i - \mathbf{x}_j, r) $$ ($$W_{spiky}$$ is symmetric), we obtain:

$$
\Delta \mathbf{x}_i = \frac{1}{\rho_0} \sum_{j = 1}^n (\lambda_i + \lambda_j)\nabla_{\mathbf{x}_i} W(\mathbf{x}_i - \mathbf{x}_j)
\tag{9}
$$

That's it, our constraint system is set up. For each particle, we set a constraint, compute its density, from there deduce its lambda and finally obtain the position correction. We can repeat it several times for each time step in order to get closer from the ideal of enforcing constant rest density everywhere in our system.

#### Simulation loop

Now that the concept of constraint and the position correction algorithm have been explicited, we can have a global overview of the simulation loop called for each time step.

<p>&nbsp;</p>
![simulationLoop](/assets/img/blog/fluids/simulationLoop.png){: loading="lazy"}
Position Based Fluids simulation loop
{:.figcaption}
<p>&nbsp;</p>

1. **Position Prediction** - We start by integrating the acceleration due to external forces to update the velocity for each particle. Using this updated value, we predict the next position of the particle. This prediction might be corrected to enforce the density constraint later on.
2. **Neighbor Search Preparation** - To avoid O(N<sup>2</sup>) time complexity (N being the number of particles), we need to spatially partition the domain and sort the particles by their cell ID. I already explained in details this process in the [boids implementation post](2021-07-11-boids.md) so I won't explain much. Simply notice the fact that we only do this processing only once for each time step, this is a performance-accuracy trade-off which works well up to a certain time step range.
3. **Position Correction** - This is the central piece of the Position Based Fluids algorithm. It is based on the constraint concept explained in the [previous section](####Introduction-to-the-concept-of-constraint). We can correct the position several iterations at each time step, using some qualitative metrics or not. In their article, Muller and Mackling iterate a fixed number of times in order to keep a constant framerate.
4. **Velocity Correction** - Once the position has been corrected and its final value reached, we recompute a second time the velocity. Note that the algorithm necessitates to maintain two separate position buffers, one at `t-1` and another one at `t`. We usually swap the two buffers at this stage to prepare the next time step iteration.
   
This is a quick overview which does not include all the specific pieces implemented for more realistic visuals. In their paper, Muller and Macklin add a **constraint force mixing (CFM) term** while computing the density constraint to avoid division by zero. Besides, they **prevent particle clustering due to a lack of neighbors by adding an artificial pressure term.** This repulsive term generates a surface tension effect which gives very interesting visual effects. They also introduce **vorticity confinement** in the last part of the simulation loop to counter the energy loss generated by the PBF framework. Finally they apply **XSPH viscosity**, coming directly from the SPH literature for a more coherent motion. **Yes, it sounds like a cooking recipe.** I quickly realized that fluids simulations in computer graphics are less rigorous than in CFD field. **You can insert some non-physical components in the equations as far as they bring something visually.**

## Implementing a Position Based Fluids model

### From the first steps of the integration...

#### Switch between different physical models

Since the beginning of the RealTimeParticles application, I had in mind to implement different physics models and to be able to switch between them through the UI. So even before beginning with the PBF model, the ground work for its integration is already half done. Thanks to classic polymorphism, safe encapsulation and the existing different subsystems APIs, it is quite easy to add a new physics model from scratch. **The tricky part comes with the OpenCL wrapper which is a singleton.** I need to be able to switch between different models using potentially similar buffers or kernels on the OpenCL device. Indeed, both boids and fluids use the spatial partioning framework and its radix sort. **I don't even want to think about the nightmare that could come out from this unwanted sharing on OpenCL device side**. The singleton should be preserved but I must clean the kernels and the buffers every time I change the physics model. This is also the opportunity to do a clean separation between the different kernels, storing them in different .cl files. The OpenCL context will call a specific subset of those files for each model.

~~~cpp
// file: "Fluids.cpp"
CL::Context& clContext = CL::Context::Get();

std::ostringstream clBuildOptions;
clBuildOptions << "-DEFFECT_RADIUS=" << Utils::FloatToStr(1.0f);

clContext.createProgram(PROGRAM_FLUIDS, { "fluids.cl", "grid.cl", "utils.cl"}, clBuildOptions.str());
~~~
A cleaner way to initiate models on OpenCL side
{:.figcaption}

#### Implementing the fluids kernels

Let's keep in mind the principle about splitting everything in small steps and start with a simplistic model without any constraint nor position correction. I simply add in my model the position prediction step, the NNS and the velocity update. Without any surprise, it basically gives me a rain-like motion. Maybe I could simply stop here, isn't it a fluid simulation after all? Hum, nevermind... time to dive into the kernels.

<p>&nbsp;</p>
![rain](/assets/img/blog/fluids/rain.gif){:.lead width="300" height="300" loading="lazy"}
Rain Simulator 2021, only for 2.99$
{:.figcaption}

The real work can begin, we have to implement the kernels used in the correction loop. After a quick review, we need one to compute the density, one to compute the density constraint \(\lambda\), another one to compute the position correction and a final kernel to correct the position. As mentioned in the graph above, the first three operations need a neighbor search, this will be particularly costly in term of performance. We only perform one for each time iteration with the boids implementation, this correction loop might run multiple times for each time step. Note that this is only for the basic implementation, without any artificial pressure or vorticity confinement tricks. But before that, we need the bricks that will be used in all these kernels: the smoothing kernel functions \(W_{poly6}\) and \(W_{spiky}\). The latter one is only called under its gradient formulation, so no need to implement it directly.

~~~c
// file: "fluids.cl"
/*
  Poly6 kernel mentioned in
  Muller et al. 2003. "Particle-based fluid simulation for interactive applications"
  Return null value if vec length is superior to effectRadius
*/
inline float poly6(const float4 vec, const float effectRadius)
{
  float vecLength = fast_length(vec);
  return (1.0f - step(effectRadius, vecLength)) * POLY6_COEFF * pow((effectRadius * effectRadius - vecLength * vecLength), 3);
}

/*
  Gradient (on vec coords) of Spiky kernel mentioned in
  Muller et al. 2003. "Particle-based fluid simulation for interactive applications"
  Return null vector if vec length is superior to effectRadius or inferior to FLOAT_EPS
*/
inline float4 gradSpiky(const float4 vec, const float effectRadius)
{
  const float vecLength = fast_length(vec);

  if(isless(vecLength, FLOAT_EPS))
    return (float4)(0.0f);

  return vec * (1.0f - step(effectRadius, vecLength)) * SPIKY_COEFF * -3 * pow((effectRadius - vecLength), 2) / vecLength;
}
~~~
$$W_{poly6}$$ and $$\nabla W_{spiky}$$ implementations
{:.figcaption}

For the rest, each kernel is relatively similar, we start by doing a NNS to retrieve the neighbors and then compute each neighbor contribution. 

~~~c
// file: "fluids.cl"
/*
  Compute fluid density based on SPH model
  using predicted position and Poly6 kernel
*/
__kernel void computeDensity(//Input
                              const __global float4 *predPos,      // 0
                              const __global uint2  *startEndCell, // 1
                              //Param
                              const     FluidParams fluid,         // 2
                              //Output
                                    __global float  *density)      // 3
{
  const float4 pos = predPos[ID];
  const uint3 cellIndex3D = getCell3DIndexFromPos(pos);

  float fluidDensity = 0.0f;

  uint cellNIndex1D = 0;
  int3 cellNIndex3D = (int3)(0);
  uint2 startEndN = (uint2)(0, 0);

  // 27 cells to visit, current one + 3D neighbors
  for (int iX = -1; iX <= 1; ++iX)
  {
    for (int iY = -1; iY <= 1; ++iY)
    {
      for (int iZ = -1; iZ <= 1; ++iZ)
      {
        cellNIndex3D = convert_int3(cellIndex3D) + (int3)(iX, iY, iZ);

        // Removing out of range cells
        if(any(cellNIndex3D < (int3)(0)) || any(cellNIndex3D >= (int3)(GRID_RES)))
          continue;

        cellNIndex1D = (cellNIndex3D.x * GRID_RES + cellNIndex3D.y) * GRID_RES + cellNIndex3D.z;

        startEndN = startEndCell[cellNIndex1D];

        for (uint e = startEndN.x; e <= startEndN.y; ++e)
        {
          fluidDensity += poly6(pos - predPos[e], fluid.effectRadius);
        }
      }
    }
  }

  density[ID] = fluidDensity;
}
~~~
Density kernel
{:.figcaption}

The density constraint and correction kernels are slightly harder to program as the maths behind are more involved but I get through it. To me, the trickiest part is how $$\nabla_{x_k} C_i$$ definition changes depending either $$k = i$$ or not. Despite that, I finally reach the point where the whole simulation loop is up and running. I fixed a multitude of small glitches in the different kernels, I double check every implementation to compare it to the theory. Everything seems to be fine, let's see how it looks like (drumroll please)...

<p>&nbsp;</p>
![firstFluid](/assets/img/blog/fluids/firstFluid.gif){:.lead width="500" height="500" loading="lazy"}
Very first capture of my "fluid" simulation
{:.figcaption}

### ... to a functioning real-time fluids simulation

In the next section, I voluntarily took a dramatic tone to highlight the tough but necessary debugging phase that so many developers encounter but don't always talk about. Don't worry, no keyboards were harmed in the making of this project. 
{:.note title="Note to the readers"}

#### Give me back my fluids particles Varus!

**So this is where the fun begins.** The theory is done, the basic implementation as well but **nothing is working as expected.** In the scene, you can discern some [Rosarch patterns](https://en.wikipedia.org/wiki/Rorschach_test) but no fluids in sight. There are **so many potential failure factors here,** that's a bit frightening. I might have misunderstood the theory, maybe there is a glitch in one kernel, a data race on GPU side, I might use wrong parameters values, buggy initial conditions. Maybe everything is working as expected and I just need to implement the specific features (artificial pressure...) to have something meaningful. Maybe this is a bit of everything. Where to begin?

Even worse, while I am trying to make sense of this nonsense particles motion, **I encounter random crashes.** GPU crashes more precisely, the worst thing I could expect. I read every kernels a dozen of times, comment whole sections of the simulation loop to identify the faulty part but **the application keeps crashing completely randomly,** sometimes after 1 second, other times after more than 2 minutes. I start seriously questioning my understanding of basically everything and at that moment, **I am getting two steps closer from an existential crisis.** Slightly stubborn, I spend most of my weekend on this issue, fascinated by these inexplicable crashes. 

It is only sunday afternoon that I find the culprit. **A small and stupid mistake far away from the complex operations.** To improve performance and reduce memory transfers, I share position buffers between OpenCL and OpenGL contexts on GPU side. The physics engine acquires the buffers, does its operations on it, releases them and gives them back to the graphics engine. To do so, **I need to explicitly acquire and release the buffers at every simulation iteration, if not we enter the land of unexpected behavior and... GPU crashes.** Well, guess what, there was a small return statement lost in the middle of my simulation loop between the acquire and release mechanisms. Depending the 
framerate of the application, **my simulation loop was going through this return without releasing the buffers and making the whole thing sink ungracefully.** On a positive note, it has forced me to add multiple safety layers and meaningful error logging on OpenCL device side.

~~~cpp
// file: "Fluids.cpp"
void Fluids::update()
{
  //...
  CL::Context& clContext = CL::Context::Get();

  // Acquire buffers from OpenGL
  clContext.acquireGLBuffers({ "p_pos" });

  if (!m_pause)
  {
    auto currentTime = clock::now();
    float timeStep = (float)(std::chrono::duration_cast<std::chrono::milliseconds>(currentTime - m_time).count()) / 16.0f;
    m_time = currentTime;

    // Skipping frame if timeStep is too large (I used it to deal with pause mode with boids)
    if (timeStep > 30.0f)
      return; // See this innocent line making everything crashing randomly when the application is too slow?

    // 30 lines of complex clContext calls...
  }

  // Release buffers to OpenGL use
  clContext.releaseGLBuffers({ "p_pos" });
}
~~~
How to waste one weekend: add an unexpected return and don't release the GPU buffers for the renderer
{:.figcaption}

#### Adjusting time and space to fluids imperatives

Now that the application is stable, I need to figure out what is going on with my particles motion. It is time to take a step back and **try to make physical sense of what I see**. The particles seem to always collapse on top of each other at the bottom of the domain, never creating any fluids layer. By chance, I remember the advice of Robert Bridson in [his great book](https://www.researchgate.net/publication/327918973_Fluid_simulation_for_computer_graphics_Second_Edition) and I realize that I wrongly use time steps in milliseconds for the integration of the external forces. Hence, **the predicted velocity is based on a gravity field three orders of magnitude too strong** and all the fluid forces become logically negligible.

>"Long experience has shown me that it is well worth keeping all quantities in a solver implicitly in SI units, rather than just set to arbitrary values." R. Bridson
{:.lead}

I also realize that I need to add **more visual indicators of the state of my simulation,** as everything is processed on the OpenCL device (my GPU) and debugging is not straightforward. I start by **connecting the color of each particle to its density.** Surprise, the particles reach rest density only when they are all compacted together at the bottom of the domain. It helps me to realize that **my domain is huge for my limited number of fluid particles** and must be reduced. I also adjust the rest density and the time step and hack my way through a dam-like scenario with a specific initial setup. Finally I can start seeing some encouraging patterns!

<p>&nbsp;</p>
![magma](/assets/img/blog/fluids/magmaDam.gif){:.lead width="500" height="500" loading="lazy"}
Magma effect in progress
{:.figcaption}

#### Focus on initial and boundary conditions

After a few tries with the particle colors, I finally settle with a nice **visual indicator based on the constraint on each particle,** from light blue (negative constraint, $$\rho < \rho_0$$) to dark blue (positive constraint, $$\rho > \rho_0$$). 

I begin to grasp the importance of the initial conditions and more particularly the **initial distance between the particles.** Contrary to the boids model whose initial arrangement was not determining much the rest of the simulation, **due to the density constraint, PBF model seem to be strongly dependent on the initial setup.** Consequently, I move away from the boids experience where users can change the number of particles interactively and implement a set of three different scenarios (dam, bomb and drop). For each scenario I manually decide the number of particles and **position them in a physically possible formation in order to see the simulation takes a realistic path.** We can see the effect of the initial inter-particle distance on the simulation with the two following gifs, **the only difference between them is the size of the initial particle domain.**

Large initial inter-particle distance  |  Small initial inter-particle distance small 
:-------------------------:|:-------------------------:|
![](/assets/img/blog/fluids/2D-bomb-large.gif)  |  ![](/assets/img/blog/fluids/2D-bomb-narrow.gif)

Finally, I need to improve the boundary conditions to have a decent basic implementation. This a complex subject for this type of simulation as it is **hard to enforce boundary conditions on a set of moving particles.** Furthermore, our whole system is based on enforcing a density close from a fixed value for every particles but **how can you reach this density value if you are against a wall and missing half the needed neighbor contributions?** The answer is you can't without assistance. Worse, the system will adjust between high and low density while trying to tackling this issue. There is a vast literature on the subject where people use ghost particles, wall functions and other tricks. At first, I start by clamping particles positions within the domain but it obviously doesn't solve the density problem. Surprisingly, adding wall functions inspired by [this implementation](https://github.com/bwiberg/position-based-fluids) and [this article](http://www.inf.ufrgs.br/cgi2007/cd_cgi/papers/harada.pdf) doesn't help much. However, apparently favored by the ancient programming gods, **I notice that the biggest artifacts disappear while clamping the corrected positions slightly inside the domain. Don't ask me why it is working because I have not the faintest idea yet,** I will probably come back on this issue later but it gives nice visuals so far so I will stick to it for now.

Without inner domain clamping          |  With inner domain clamping  
:-------------------------:|:-------------------------:|
![](/assets/img/blog/fluids/2D-BC-broken.gif)  |  ![](/assets/img/blog/fluids/2D-BC-fixed.gif)

#### Advanced features

Once the basic implementation is properly functioning, we can add the niceties to improve the overal behavior and the visuals. Nevertheless, they can greatly affect the performance so their usage is a visual-framerate trade-off to keep in mind.

The constraint force mixing (CFM) used to prevent a division by zero when computing density constraint is a must, it costs virtually nothing in term of computation and add some stability to the simulation.

Adding the artificial pressure term is more involved in term of implementation and performance. It requires noticable additional computations at the position correction kernel level which can impact framerate. However, the visual improvements are impressive and totally worth it. By encouraging the particles to stay into a single pack, it generates a compelling surface tension effect. 

Basic mode            |  BM + Artificial Pressure  
:-------------------------:|:-------------------------:|
![](/assets/img/blog/fluids/2D-Dam-basic.gif)  |  ![](/assets/img/blog/fluids/2D-Dam-artPressure.gif)

The last feature proposed in the article is the hardest and heaviest to implement. We have to compute the vorticity for each particle at every frame, use it to compute the vorticity confinement that we apply on the velocity once integrated and finally smooth the velocity field by applying the XSPH viscosity correction to balance the added vorticity energy. It requires three different neighbors search and three new kernels, this is very consuming. For my biggest 3D scenario, a dam with 65k particles, the slowdown is  strong enough to be noticeable but for 2D cases it brings interesting effects with acceptable performance cost.

Basic mode (30fps)           |  BM + Artificial Pressure (20fps) | BM + AP + Vorticity + XSPH (16-20fps)
:-------------------------:|:-------------------------:|:-------------------------:|
![](/assets/img/blog/fluids/3D-Dam-basic.gif)  |  ![](/assets/img/blog/fluids/3D-Dam-artPressure.gif)  |  ![](/assets/img/blog/fluids/3D-Dam-vorticity.gif)

Here is the final rendering for the classical 3D dam scenario. The 130k particles are processed in real-time and the model uses all the available advanced features. Notice the boundary artifact with an unexpected high amplitude wave in the back when the main wave breaks off at the end. I will need to look at those boundary particles at some point in the future but I am already very pleased by the overall result. Sorry for the quality of gif, I had to reduced it in order to make it smaller in size.

<p>&nbsp;</p>
![finalDam](/assets/img/blog/fluids/3D-Dam-final.gif){:.lead width="500" height="500" loading="lazy"}
3D dam scenario 130k - BM + AP + Vorticity + XSPH (16 - 20fps)
{:.figcaption}


## Conclusion
So here we are, finally reaching the conclusion of this project! I am really very happy to have implemented successfully this captivating Position Based Fluids model. This fluids simulation project has been in my mind since I finished my master thesis, more than 5 years ago! Overall, it took me about 4 weeks, reading papers, programming and debugging my way out. During this journey, I also took some time ensuring everything was functional with the boids despite the tons of changes and making everything coherent at the application level. It was totally worth it, everything is interactive and functional. In the end, I spent my last couple of days on cleaning the codebase and automating the installer packaging with CPack and voila! We now have a [RealTimeParticle 1.0 installer](https://github.com/axoloto/RealTimeParticles/releases/tag/RTP_1.0) working with Windows x64 OS.

As always, there is still a lot of work in the backlog. I could focus on the rendering side, to implement proper free surfaces and nice lights effect. The boundary conditions could also benefit from an upgrade. However, I am thinking about moving away from particles system for a while and discover new things. I will probably dive into CUDA in a short term future, in order to have access to proper debugging and performance tools and improve my GPGPU skills.

Thanks for reading up to this point! Feel free to contact me if you have any question.

RealTimeParticles codebase is open-source and available [here](https://github.com/axoloto/RealTimeParticles).
