---
layout: post
title: gpu fluids simulator
description: >
  Implementation of a Position Based Fluids real-time simulator
  
  **Source code available [here](https://github.com/axoloto/RealTimeParticles).**
sitemap: false
hide_last_modified: false
category : rtp
image: /assets/img/blog/fluids/fluids-bg.jpg
accent_image: 
  background: url('/assets/img/blog/fluids/fluids-bg.jpg') center/cover
  overlay: false
accent_color: '#ccc'
theme_color: '#ccc'
---

# Position Based Fluids

1. this list will be replaced by the table of contents
{:toc}

## Introduction

To acquire knowledge and expertise in a domain, I tend to mostly rely on two mechanisms:
1. **Aiming high** (sometimes stupidly high)
2. **Splitting complex challenges in smaller ones to tackle them separately**

Following and combining those two basic principles usually works pretty well for me. I set high goals far ahead, partition the path leading to it in a multitude of intermediary steps, and patiently reach everyone of them to finally arrive at the ultimate objective.

Hence the boids behaviour model described in my latest [post](https://axoloto.github.io/rtp/2021-07-11-boids/) was just a step. I greatly enjoyed it but it was first of all a necessary intermediary stage. I had something more complicated in mind, a physical model like fluids or combustion, but I wanted to separate theorical and technical challenges.

Boids project was all about setting up a C++ project from scratch, creating an OpenGL render, an OpenCL wrapper and connecting them altogether. No significant theorical obstacles but a lot of technical difficulties to handle one by one. Real physical models usually come with complex ordinary or partial differential equations (ODE/PDE), adding another mathematical layer of complications. I wanted to have strong foundations before playing with it. 

Well guess what; boids simulation being fully functional, now is the time for the real game: **implementing a real-time 3D fluids simulation**! Let's begin.

## Simulating Fluids with Math and bits

**I always found amazing that such a complicated thing like fluids dynamics can be modelized by a couple of differential equations.** Fluids motions can be incredibly complex, both temporally and spatially, nevertheless some 19<sup>th</sup> century geniuses, namely Claude-Louis Navier and George Gabriel Stokes, brought up a pair of PDEs and showed that by approximately solving them you could get very close from many fluids behaviors present in our world. Crazy, right? I say approximately because the Navier-Stokes equations have no evident solutions in 3D and therefore are solved using numerical or analytical models based on different approximations. **So, before implementing anything, we need to decide which of those approaches suits the best our needs.**

By the way, if you have 3D solutions of Navier-Stokes equations written on your fridge, don't hesitate to share it to the world because it is among the [seven most wanted open problems in mathematics](http://www.claymath.org/millennium-problems/navier%E2%80%93stokes-equation) ($1 million prize).

$$
\begin{aligned}
\nabla \cdot \mathbf{u} &= 0 \\
\rho ( \frac{\partial \mathbf{u}}{\partial t} + \mathbf{u} \cdot \nabla \mathbf{u} ) &= - \nabla p + \rho \mathbf{g} + \mu \nabla^2 \mathbf{u}
\end{aligned}
$$

*Miss Equation 1822, the Navier-Stokes equations (Eulerian description for a viscous incompressible fluid)*

### Eulerian or Lagrangian? To be or not to be?

Before diving into numerical schemes and solvers, first we need to choose between two different ways of looking at fluids motion:
1. **Eulerian description** - **We look at the fluids domain globally, considering it as a vector field.** The fluids behavior is analyzed and solved through time at specific fixed places in the studied domain. This approach is extensively used in the **computer fluids dynamics** (CFD) field where scientists and engineers apply it to analyze and solve real world fluids problems, for example reducing aircraft drag or improving wind turbines efficiency. **This approach can gives very precise estimations of flow motions and forces.** I used it in my master thesis as most of the Fluids Lab I was working at. Unfortunately **it can be extremely costly**, some 3D heavy simulations can literally take weeks or months to proceed. Besides, particles movements can be obtained by post-processing the fluids velocity field but they are not directly considered in the solver.

2. **Lagrangian description** - **We look at the fluids as a group of individual particles, analyzing their interactions to determine their trajectories and conditions.** Contrary to the Eulerian approach, the domain is not considered as a whole and its importance is less significant. Hence, no need of mesh or grid, simply a bunch of particles on which we apply some rules modelizing the fluids interaction. To make it simple, **it is a bit like approximating a fluids domain with a ball pit**. Historically, Lagrangian simulations have been considered less precise than Eulerian ones. One of their biggest forces is their execution speed, they **can be used for coarse but fast and visually acceptable simulations**. Thus, they are particularly apprecieted in the world of **computer graphics**. By being meshfree, it is also well suited for free-surface problems or ones with large boundary displacements.

In short, by using Lagrangian description we can favor interactivity over accuracy. **Hence, a Lagrangian solver clearly sounds as the right candidate for our real-time particle simulator.** We "simply" need to apply the right combinations of rules on each particles to give them a realistic visual sense of fluids motion. This is the topic of the next part.

### Lagrangian computational methods

#### The old venerable master

**Smooth Particles Hydrodynamics (SPH)** is one of the major computational methods used in Computer Graphics for fluids. Initially developped by Gingold, Monaghan and Lucy in 1977 for astronomical problems (article), since then **it has been extensively used in fluids simulations, for its simplicity of implementation and execution**. Combined with some post-processing rendering, it can deliver very realist free-surface interactive simulations.

**SPH method divides the fluid continuum in a set of discrete particles.** For each particle, SPH interpolates its fluids properties using its neighbors and functions quantifying the influence of each of them. Called **smoothing kernel functions**, those functions are a central component of the method. They determine how particles interact with each others and must be carefuly designed. Using this method, we can **compute pressure and viscosity forces on each particle and deduce its acceleration through Newton's second law.** Then, its integration gives us the velocity and finally the position of the particle. To facilitate nearest neighbor search (NNS), SPH method can also use some spatial partioning. Particles, sum of neighbors contributions, spatial partioning... sounds familiar, right? Don't be surprised, **it is strongly similar to the existing implementation used for the boids model and that's very good news.**

\begin{equation}
 A(\mathbf{x}) = \sum_j m_j \frac{A_j}{\rho_j} W(\mathbf{x} - \mathbf{x_j}, h)\end{equation}

*Interpolation at $$\mathbf{x}$$ of a scalar quantity A by a weighted sum of contributions from its neighbor particles $$j$$*

$$
\begin{aligned}
W_{poly6}(\mathbf{r}, h) = \frac{315}{64 \pi h^9} 
      & \left\{
    \begin{array}{lr}
      (h^2 -\Vert r \Vert^2)^3 & 0 \leq r \leq h\\
      0 & otherwise
    \end{array}
    \right.    \\
W_{spiky}(\mathbf{r}, h) = \frac{15}{\pi h^6} & \left\{
     \begin{array}{lr}
      (h - \Vert r \Vert)^3 & 0 \leq r \leq h\\
      0 & otherwise
    \end{array}
    \right. 
\end{aligned}
$$

*Exemple of smoothing kernel functions used in our model and mentioned in [Muller et al., 2001](https://matthias-research.github.io/pages/publications/sca03.pdf)*

Despite its charms, after reading a few papers on SPH, **I realized that it might not be the perfect solution.** **Traditional SPH model can rapidly shows instabilities when some particles have few or no neighbors, injecting negative pressure in the domain.** There is a vast number of variants made in the past decades to address those issues and improve the initial model, PCISPH, XSPH... Modern SPH results are much better, but the **use of small time steps for the simulation is still necessary, limiting simulation speed.** Hence, even if you manage to have a simulation running at 60 fps, the motion of particles might be visually slow.

#### A newcomer in town

While reading a well-known fluids SPH paper called [Particle-Based Fluid Simulation for Interactive Applications](https://matthias-research.github.io/pages/publications/sca03.pdf) written in 2001, one authors name caught my attention: Matthias Muller. He is currently the physics research lead at NVIDIA, and I remembered that I had seen another computational method on his personal page a while ago. 

Called **Position Based Dynamics** (PBD), this framework was presented in 2006 and has been used for fluids simulation in a paper named [**Position Based Fluids**](https://mmacklin.com/pbf_sig_preprint.pdf) (PBF). It was published in 2013 by Muller and Macklin, 12 years after the initial SPH paper. Personal rule of thumb; **when scientists wrote a paper on the same subject 10 years after the previous one, it is usually a good idea to read it**, they have probably something better to propose. This PBF paper brought indeed great insights to this new method but also highlighted the limitations of the SPH model. It also helped me understanding the differenciation between two keywords used in this field: **interactive vs real- time**. The first one means fast enough to play with interactively but potentially still visually too slow to be acceptable in many applications, the latter means interactive and fast enough for video games type application, exactly what we are looking for.

Many computation methods, SPH included, are based on Newton's second law. You compute the forces applied on each particle, use it to obtain its acceleration and from there integrate it twice to get its position. You repeat it for each time step and you have something moving. Position Based Dynamics/Fluids approach is radically different, you don't even compute the acceleration, you mostly focus on **particles position** and **constraints**.

Constraints are designed functions defining how you want the system to behave, how you want the particles to be positioned between each other. You can imagine one constraint as a web connecting one particle to its neighbors and constraining them in order to have some physical property (energy, density or else) enforced at its desired value. I find this approach very interesting. No double time integration and more explicit control on your system, as you manipulate directly particle positions. To improve precision and get closer to the wanted behavior, you can iterate on your constraint system at each time step. By doing so, PBD brings more stability and allows us to increase simulation time steps, **bringing us into the real-time processing world.**

### Position Based Fluids equations

#### Introduction to constraints
In their [article](https://mmacklin.com/pbf_sig_preprint.pdf), Macklin and Muller give a good overall presentation of their proposed framework. It is quite consequent and contains different specific ramifications and fine-tuning so I won't go into every details. Instead, **I want to focus on the key concept of constraint** because it took me a while to understand it completely and I think its comprehension is vital for any PBF implementation.

For each particle $$ p_i $$, Position Based Fluids approach sets a single positional constraint $$ C_i $$ on $$ p_i $$ itself and its neighbors, all their positions being contained in $$ \mathbf{X} = [\mathbf{x}_1, \ldots, \mathbf{x}_n]$$ with $$ i \in [1, \ldots, n] $$. The objective is to constraint each particle to reach a position $$ \mathbf{x}_i $$ where its density $$ \rho_i $$ tends towards the rest density $$ \rho_0 $$, a fluids-dependent property defined by the user.

$$ C_i(\mathbf{X}) = \frac{\rho_i}{\rho_0} - 1  \tag{1} $$

Interestingly, we rely on a SPH kernel function $$ W $$ to compute $$ \rho_i $$. In the article, they use the smoothing kernels $$ W_{poly6} $$ and $$ W_{spiky} $$ introduced previously.

$$ \rho_i = \sum_{j = 1}^n W_{poly6}(\mathbf{x}_i - \mathbf{x}_j, r) \tag{2} $$

For every constraint, we want to find the set of positions $$ \mathbf{X} $$ which forces the density $$ \rho_i $$ to be equal to $$ \rho_0 $$. Mathematically, it means that we want to find the particles positions correction $$ \Delta \mathbf{X}^i $$ giving us:

$$ C_i(\mathbf{X} + \Delta \mathbf{X}^i) = 0 \tag{3} $$

From there, we can try to minimize the constraint $$ C_i $$ by iterations with a gradient descent. To do so, we linearly approximate $$ (3) $$ and assume that $$ \Delta \mathbf{X}^i $$ is in the direction of the negative gradient of $$ C_i $$. Considering $$ \nabla C_i = \nabla_{\mathbf{X}} C_i = [\nabla_{\mathbf{x}_1} C_i, \ldots, \nabla_{\mathbf{x}_n} C_i]$$ :

$$ \Delta \mathbf{X}^i \approx \nabla C_i(\mathbf{X}) \lambda_i \tag{4} $$

$$
\begin{aligned}
C_i(\mathbf{X} + \Delta\mathbf{X}^i) & \approx C_i(\mathbf{X}) + \nabla C_i(\mathbf{X})^T  \Delta\mathbf{X} = 0  \\
      & \approx C_i(\mathbf{X}) + \nabla C_i(\mathbf{X})^T \nabla C_i(\mathbf{X}) \lambda_i = 0
\end{aligned} \tag{5}
$$

At this point, we can deduce the value of the density constraint coefficient $$ \lambda_i $$ :

$$ \lambda_i = - \frac{C_i(\mathbf{X})}{\sum_{j = 1}^n {\mid \nabla_{\mathbf{x}_j} C_i(\mathbf{X}) \mid}^2 } \tag{6} $$ 

Besides, using the SPH discrete definition of the density $$ (2) $$, we can define more precisely the gradients of $$ C_i $$ with respect to each particle:

$$
\nabla_{\mathbf{x}_k} C_i(\mathbf{X}) = \frac{1}{\rho_0} \left\{ 
    \begin{aligned}
     & \sum_{j = 1}^n \nabla_{\mathbf{x}_i} W_{spiky}(\mathbf{x}_i - \mathbf{x}_j, r), \space k = i \\ 
     & - \nabla_{\mathbf{x}_j} W_{spiky}(\mathbf{x}_i - \mathbf{x}_j, r), \space k = j
\end{aligned} \right. \tag{7}
$$

Remember that for each particle we set one constraint **affecting this particle but also its neighbors.** Hence for each particle, **its position is corrected by its own constraint but also the ones set on its neighborhood.** Macklin and Muller superpose each contribution:

$$
\Delta \mathbf{x}_i = \sum_{j = 1}^n \Delta \mathbf{x}^j_i = \sum_{j = 1}^n  \lambda_j \nabla_{\mathbf{x}_i} C_j(\mathbf{X}) \tag{8}
$$

Finally, by connecting $$ (7) $$ and $$ (8) $$ and assuming $$ \nabla_{\mathbf{x}_i} W_{spiky}(\mathbf{x}_i - \mathbf{x}_j, r) = -\nabla_{\mathbf{x}_j} W_{spiky}(\mathbf{x}_i - \mathbf{x}_j, r) $$ ($$W_{spiky}$$ being symmetric), we obtain:

$$
\Delta \mathbf{x}_i = \frac{1}{\rho_0} \sum_{j = 1}^n (\lambda_i + \lambda_j)\nabla_{\mathbf{x}_i} W(\mathbf{x}_i - \mathbf{x}_j)
\tag{9}
$$

That's it, our constraint system is set up. For each particle, we set a constraint, compute its density, deduce its lambda and obtain the position correction from there. We will repeat it several times at each time step in order to get closer from the ideal of enforcing constant rest density everywhere in our system.

#### Simulation loop

Now that the position correction algorithm has been explicited, we can do a global overview of the simulation loop called at each time step.

<p>&nbsp;</p>
![simulationLoop](/assets/img/blog/fluids/simulationLoop.png){: loading="lazy"}
Position Based Fluids simulation loop
{:.figcaption}
<p>&nbsp;</p>

1. **Position Prediction** - First we update the velocity of each particle by integrating its acceleration due to external forces. Using this updated value, we predict the next position of the particle. **This prediction will be corrected later in (4) to enforce the density constraint.**
2. **Nearest Neighbor Search Preparation** - To avoid O(N<sup>2</sup>) time complexity (N being the number of particles), we spatially partition the domain and sort the particles by their cell ID. I already explained this process in the [boids implementation post](2021-07-11-boids.md) so I won't go into details. Simply notice the fact that **we do this processing only once for each time step,** this is a performance-accuracy trade-off working well up to a certain time step range.
3. **Position Correction** - Central piece of the Position Based Fluids algorithm, it is based on the constraint concept explained in the [previous section](####Introduction-to-the-concept-of-constraint). **We correct the position in multiple iterations at each time step.** To decide when to stop the correction, we can use some qualitative metrics or simply set a fixed number of iterations. In their article, Muller and Mackling iterate a constant number of times in order to keep a regular framerate.
4. **Velocity Correction** - Once the position has been corrected, we recompute a second time the velocity based on this new value. Note that the algorithm necessitates to maintain two separate position buffers, one at `t-1` and another one at `t`. We usually swap the two buffers at this stage to prepare the next time step iteration.
   
This is a quick overview which does not include all the specific pieces implemented for more realistic visuals. In their paper, Muller and Macklin add a **constraint force mixing (CFM) term** while computing the density constraint to avoid division by zero. They add an **artificial pressure term to prevent particle clustering due to a lack of neighbors**. This repulsive term generates a surface tension effect which gives very interesting visual effects. They also introduce some **vorticity confinement** in the last part of the simulation loop to counter the energy loss generated by the PBF framework. Finally, they apply **XSPH viscosity**, a concept coming directly from the SPH literature which produces a more coherent motion. **Yes, all of this sounds like a cooking recipe.** I quickly realized that fluids simulations in computer graphics are less rigorous than in CFD field. **As long as it brings something visually, you can insert some non-physical components in the equations.**

## Implementing a Position Based Fluids model

### From the first steps of the integration...

#### A switch between different physical models

Since the beginning of the RealTimeParticles application, I have planned to implement different physical models and to switch between them through the UI. So, even before working on the PBF model, the ground work for its integration is already halfway. Thanks to classic polymorphism, secure encapsulation and different existing subsystems APIs, it is quite trivial to add a new physical model from scratch. **The tricky part comes from the OpenCL wrapper being a singleton.** I need to be able to switch between different models using potentially similar buffers or kernels on the OpenCL device. For example, both boids and fluids models use the spatial partioning framework and its radix sort. **I don't even want to think about the nightmare that could arise from this unwanted cohabitation on the OpenCL device side**. The singleton should be preserved but I have to clean the kernels and the buffers every time I change my physical model. This is also a good opportunity to do a clean separation between the different kernels by storing them in separated .cl files. The OpenCL context will now call a specific subset of those files for each model.

~~~cpp
// file: "Fluids.cpp"
CL::Context& clContext = CL::Context::Get();

std::ostringstream clBuildOptions;
clBuildOptions << "-DEFFECT_RADIUS=" << Utils::FloatToStr(1.0f);

clContext.createProgram(PROGRAM_FLUIDS, { "fluids.cl", "grid.cl", "utils.cl"}, clBuildOptions.str());
~~~
A cleaner way to initiate models on OpenCL side
{:.figcaption}

#### Implementing the fluids kernels

Let's put into practice the principle of splitting everything into small steps. I will start with a simplistic model, without constraint or correction of position. I simply assemble a skeleton with the position prediction step, the NNS and the velocity update. **Unsurprisingly, it essentially gives me rain-like motion.** Maybe I could just stop here, isn't it a fluid simulation after all? Hum, nevermind... time to dive into the kernels.

<p>&nbsp;</p>
![rain](/assets/img/blog/fluids/rain.gif){:.lead width="300" height="300" loading="lazy"}
Rain Simulator 2021, only for 2.99$
{:.figcaption}

The real work can begin, we have to implement the kernels used in the correction loop. After a quick review, I count four kernels to generate; one to compute the density, one for the density constraint $$\lambda$$, another one for the position correction and a final one to correct the position. As mentioned in the graph above, **the first three operations need a neighbor search so we might run neighbors searches a small dozen times for each time step**. **This will be significantly costly in terms of performance.** With the boids implementation, we only performed this operation a single time at each time iteration. Besides, note that this is only for the basic implementation, without any artificial pressure or vorticity confinement tricks.

But before analyzing performance, we need the bricks that will be used in all these kernels: the smoothing kernel functions $$W_{poly6}$$ and $$W_{spiky}$$. The latter one is only called under its gradient formulation, so no need to implement it directly. Here are the proposed implementations:

~~~c
// file: "fluids.cl"
/*
  Poly6 kernel mentioned in
  Muller et al. 2003. "Particle-based fluid simulation for interactive applications"
  Return null value if vec length is superior to effectRadius
*/
inline float poly6(const float4 vec, const float effectRadius)
{
  float vecLength = fast_length(vec);
  return (1.0f - step(effectRadius, vecLength)) * POLY6_COEFF * pow((effectRadius * effectRadius - vecLength * vecLength), 3);
}

/*
  Gradient (on vec coords) of Spiky kernel mentioned in
  Muller et al. 2003. "Particle-based fluid simulation for interactive applications"
  Return null vector if vec length is superior to effectRadius or inferior to FLOAT_EPS
*/
inline float4 gradSpiky(const float4 vec, const float effectRadius)
{
  const float vecLength = fast_length(vec);

  if(isless(vecLength, FLOAT_EPS))
    return (float4)(0.0f);

  return vec * (1.0f - step(effectRadius, vecLength)) * SPIKY_COEFF * -3 * pow((effectRadius - vecLength), 2) / vecLength;
}
~~~
$$W_{poly6}$$ and $$\nabla W_{spiky}$$ implementations
{:.figcaption}

For the rest, each kernel is relatively similar, we start by doing a NNS to retrieve the neighbors and then compute each neighbor contribution. 

~~~c
// file: "fluids.cl"
/*
  Compute fluid density based on SPH model
  using predicted position and Poly6 kernel
*/
__kernel void computeDensity(//Input
                              const __global float4 *predPos,      // 0
                              const __global uint2  *startEndCell, // 1
                              //Param
                              const     FluidParams fluid,         // 2
                              //Output
                                    __global float  *density)      // 3
{
  const float4 pos = predPos[ID];
  const uint3 cellIndex3D = getCell3DIndexFromPos(pos);

  float fluidDensity = 0.0f;

  uint cellNIndex1D = 0;
  int3 cellNIndex3D = (int3)(0);
  uint2 startEndN = (uint2)(0, 0);

  // 27 cells to visit, current one + 3D neighbors
  for (int iX = -1; iX <= 1; ++iX)
  {
    for (int iY = -1; iY <= 1; ++iY)
    {
      for (int iZ = -1; iZ <= 1; ++iZ)
      {
        cellNIndex3D = convert_int3(cellIndex3D) + (int3)(iX, iY, iZ);

        // Removing out of range cells
        if(any(cellNIndex3D < (int3)(0)) || any(cellNIndex3D >= (int3)(GRID_RES)))
          continue;

        cellNIndex1D = (cellNIndex3D.x * GRID_RES + cellNIndex3D.y) * GRID_RES + cellNIndex3D.z;

        startEndN = startEndCell[cellNIndex1D];

        for (uint e = startEndN.x; e <= startEndN.y; ++e)
        {
          fluidDensity += poly6(pos - predPos[e], fluid.effectRadius);
        }
      }
    }
  }

  density[ID] = fluidDensity;
}
~~~
Density kernel
{:.figcaption}

The density constraint and correction kernels are slightly harder to implement as the maths behind are a bit more involved but it is still doable. The trickiest part is how $$\nabla_{x_k} C_i$$ definition changes depending on whether $$k = i$$ or not. Despite those small complications, **I finally get to the point where the whole simulation loop is ready to roll.** Before runnint it, I fix a multitude of glitches in the different kernels, double check every implementation by comparing it to the theory. Alright, everything seems coherent, let's see how it looks like (drumroll please)...

<p>&nbsp;</p>
![firstFluid](/assets/img/blog/fluids/firstFluid.gif){:.lead width="500" height="500" loading="lazy"}
Very first capture of my "fluid" simulation, nice try
{:.figcaption}

### ... to a functioning real-time fluids simulation

*In the next section, I intentionally take a dramatic tone to highlight the tough but necessary debugging phase that so many developers encounter but don't always talk about. Don't worry, no keyboards were harmed in the making of this project.*
{:.note title="Note to the readers"}

#### Give me back my fluids particles Varus!

**So this is where the fun begins.** Theory, checked. Basic implementation, checked. Yet, **nothing is working as expected.** In the scene shown above, you can discern some [Rosarch patterns](https://en.wikipedia.org/wiki/Rorschach_test) but no fluid in sight. There are **so many potential failure factors here** that it is a bit frightening. Maybe I misunderstood the theory, maybe there is a bug in a secondary kernel or even a data race on GPU side. I might use wrong parameters values or buggy initial conditions. Maybe everything is working as expected and I just need to implement the specific features (artificial pressure...) to have something meaningful. Maybe it is a bit of everything. Endless posibilities. Where to begin?

Worse yet, while I am trying to make sense of this nonsense particles motion, **I encounter random crashes.** GPU crashes more precisely, the worst thing I could expect. I read every kernels a dozen of times, comment whole sections of the simulation loop to identify the faulty part but **the application keeps crashing completely randomly,** sometimes after 1 second, other times after more than 2 minutes. I start seriously questioning my understanding of pretty much everything and **getting two steps closer from an existential crisis.** Slightly stubborn, I spend most of my weekend on this issue, obsessed by these inexplicable crashes. 

It is only Sunday afternoon that I find the culprit. **A stupid little mistake far away from the complex operations.** To improve performance and reduce memory transfers, I share position buffers between OpenCL and OpenGL contexts on the GPU side. This is called **OpenCL-OpenGL interoperability.** The physics engine acquires the buffers, performs its operations on them, frees them and returns them to the graphics engine. To do so, **I need to explicitly acquire and release the buffers on each iteration of the simulation in the physics engine, if not we enter the land of unexpected behavior and... GPU crashes.** Well guess what, there is a small return statement lost in the middle of my simulation loop between the acquire and release mechanisms. Depending the framerate of the application, **my simulation loop goes through this return without releasing the buffers and makes the whole thing collapse ungracefully.** On a positive note, it forces me to add multiple safety layers and meaningful error logging on OpenCL device side.

~~~cpp
// file: "Fluids.cpp"
void Fluids::update()
{
  //...
  CL::Context& clContext = CL::Context::Get();

  // Acquire buffers from OpenGL
  clContext.acquireGLBuffers({ "p_pos" });

  if (!m_pause)
  {
    auto currentTime = clock::now();
    float timeStep = (float)(std::chrono::duration_cast<std::chrono::milliseconds>(currentTime - m_time).count()) / 16.0f;
    m_time = currentTime;

    // Skipping frame if timeStep is too large (I used it to deal with pause mode with boids)
    if (timeStep > 30.0f)
      return; // See this innocent line making everything crashing randomly when the application is too slow?

    // 30 lines of complex clContext calls...
  }

  // Release buffers to OpenGL use
  clContext.releaseGLBuffers({ "p_pos" });
}
~~~
How to waste one weekend: add an unexpected return and don't release the GPU buffers for the renderer
{:.figcaption}

#### Adjusting time and space to fluids imperatives

Now that the application is stable, I need to figure out what is going on with my particles motion. It is time to take a step back and **try to make physical sense of what I see**. The particles seem to always collapse on top of each other at the bottom of the domain, never creating any fluids layer. By chance, I remember the advice of Robert Bridson in [his great book](https://www.researchgate.net/publication/327918973_Fluid_simulation_for_computer_graphics_Second_Edition) and I realize that I use wrong units for the integration of the external forces. By integrating it with time steps in milliseconds, **the predicted velocity is based on a gravity field that is three orders of magnitude too strong** and all the internal fluid forces become negligible.

>*"Long experience has shown me that it is well worth keeping all quantities in a solver implicitly in SI units, rather than just set to arbitrary values."* R. Bridson
{:.lead}

I also realize that I need to add **more visual indicators of the state of my simulation,** as everything is processed on the OpenCL device (my GPU) and debugging is not straightforward. I start by **connecting the color of each particle to its density.** Surprise, the particles reach rest density only when they are all compacted together at the bottom of the domain. It helps me realize that **my domain is huge for my limited number of fluid particles** and needs to be reduced. I also adjust the rest density and the time step and work my way through ro implement a dam-like scenario with a specific initial setup. Finally I can start seeing some encouraging patterns!

<p>&nbsp;</p>
![magma](/assets/img/blog/fluids/magmaDam.gif){:.lead width="500" height="500" loading="lazy"}
Magma effect in progress, still strong boundary artifacts
{:.figcaption}

#### Focus on initial and boundary conditions

After a few tries with the particle colors, I am finally satisfied with a nice **visual indicator based on the constraint on each particle,** from light blue (negative constraint, $$\rho < \rho_0$$) to dark blue (positive constraint, $$\rho > \rho_0$$). 

Besides, I begin to grasp the importance of the initial conditions and more particularly the **initial distance between the particles.** Contrary to the boids model whose initial arrangement does not determine much the rest of the simulation, **PBF model seems strongly dependent on the initial setup because of the density constraint.** Consequently, I move away from the boids workflow where users can change the number of particles interactively and implement a set of three different scenarios (dam, bomb and drop) for the fluids case. For each scenario I manually decide the number of particles and **position them in a physically possible formation in order to help the simulation taking a realistic path.** We can see the effect of the initial inter-particle distance on the simulation with the two following examples, **the only difference between them is the size of the initial particle domain.**

Large initial inter-particle distance  |  Small initial inter-particle distance small 
:-------------------------:|:-------------------------:|
![](/assets/img/blog/fluids/2D-bomb-large.gif)  |  ![](/assets/img/blog/fluids/2D-bomb-narrow.gif)

Finally, I need to improve the boundary conditions to have the standard implementation being fully functional. This is a complex subject for this type of simulation as it is **hard to enforce boundary conditions on a set of moving particles.** Furthermore, our constraint system is based on enforcing a similar density to every particles but **how can you reach this same density value if you are against a wall and missing half your neighors and their needed contributions?** Worse yet, the system locally oscillates between high and low density while trying to handle this issue, making it unstable. There is a vast literature on the subject where people use ghost particles, wall functions and other tricks at the boundaries. At first, I start by clamping particles positions within the domain but it doesn't solve the density problem. Surprisingly, adding wall functions inspired by [this implementation](https://github.com/bwiberg/position-based-fluids) and [this article](http://www.inf.ufrgs.br/cgi2007/cd_cgi/papers/harada.pdf) doesn't help much, I still see the artifacts. However, apparently favored by the ancient programming gods, **I notice that the biggest unstabilities disappear while clamping the corrected positions slightly inside the domain and not at its exact borders. Don't ask me why this trick works because I don't have a clue yet,** I will probably come back to this issue later but it gives nice visuals so far so I will stick with it for now.

Without inner domain clamping          |  With inner domain clamping  
:-------------------------:|:-------------------------:|
![](/assets/img/blog/fluids/2D-BC-broken.gif)  |  ![](/assets/img/blog/fluids/2D-BC-fixed.gif)

#### Advanced features

Once the basic implementation functions properly, we can add all the niceties to improve its overal behavior and the visuals. Yet, these additional features can greatly affect the performance so their usage is an esthetic-performance trade-off and we should not abuse their use.

- **Constraint force mixing** (CFM) - It is used to prevent a division by zero when computing density constraint is a must, it costs virtually nothing in term of computation and add some stability to the simulation.

- **Artificial pressure** - Adding this term is more involved in term of implementation and performance. It requires additional computations in the position correction kernel which can noticably impact framerate. However, the visual improvements are impressive and worth it. By encouraging the particles to stay into a single pack, **it generates a compelling surface tension effect**. 

Basic mode            |  BM + Artificial Pressure  
:-------------------------:|:-------------------------:|
![](/assets/img/blog/fluids/2D-Dam-basic.gif)  |  ![](/assets/img/blog/fluids/2D-Dam-artPressure.gif)

- **Vorticity confinement and XPSH viscosity** - The **most costly and difficult feature to implement.** In their article, Macklin and Muller propose to compute each particle vorticity and use it to compute the **vorticity confinement** at every frame. We can then integrate and add this term to the particle velocity. It will increase velocity field where vorticity is present, adding some energy to active fluids area. Finally, we need to smooth the velocity field by applying the **XSPH viscosity** correction to balance the added vorticity energy. The whole process requires three different neighbors search and three new kernels, making it very costly. For my biggest 3D scenario, a dam with 130k particles, the slowdown is strong enough to be annoying but for 2D cases it brings interesting effects for an acceptable performance cost.

Basic mode (30fps)           |  BM + Artificial Pressure (20fps) | BM + AP + Vorticity + XSPH (16-20fps)
:-------------------------:|:-------------------------:|:-------------------------:|
![](/assets/img/blog/fluids/3D-Dam-basic.gif)  |  ![](/assets/img/blog/fluids/3D-Dam-artPressure.gif)  |  ![](/assets/img/blog/fluids/3D-Dam-vorticity.gif)

I added below the final rendering for the classical 3D dam scenario. Sorry for the quality of gif, I had to reduced it in order to make it smaller in size. The **130k particles are processed in real-time** and the model uses all the available advanced features. Notice the boundary artifacts including an unexpected high amplitude structure in the back when the main wave breaks off at the end. I will need to look at those boundary particles at some point in the future but I am already very pleased by the overall result. 

<p>&nbsp;</p>
![finalDam](/assets/img/blog/fluids/3D-Dam-final.gif){:.lead width="500" height="500" loading="lazy"}
3D dam scenario 130k - BM + AP + Vorticity + XSPH (16 - 20fps)
{:.figcaption}


## Conclusion
Finally, this long-awaited project is coming to an end! **I am really very happy to have implemented successfully this exciting Position Based Fluids model.** This idea of implementing a fluids simulation has been stuck in my head since I finished my master thesis, more than 5 years ago! All in all, **it took me about 4 weeks,** reading papers, implementing it and debugging my way out. During this journey, I also took some time to make sure everything was coherent and the boids model still functional despite the tons of changes. In the end, I spent the last couple of days on cleaning the codebase, **automating the installer packaging with CPack** and voila! We now have a [RealTimeParticle 1.0 installer](https://github.com/axoloto/RealTimeParticles/releases/tag/RTP_1.0) running on Windows x64 OS.

As always, there is still a lot of work in the backlog. I could spend more time on the rendering process to implement proper free fluids surfaces and nice lights effect. The boundary conditions could also benefit from an upgrade. Nevertheless, I am happy with the current state of the application and **I am thinking about moving away from particles system for a while and experiment new things.** I will probably dive into CUDA in a short term future, to have access to proper GPU performance tools and improve my GPGPU skills.

**Thanks for reading up to this point! Feel free to contact me if you have any question.**

PS: RealTimeParticles codebase is open-source and available [here](https://github.com/axoloto/RealTimeParticles).
