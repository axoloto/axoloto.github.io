---
layout: post
title: gpu boids simulator
description: >
  The story behind the implementation of a GPU (OpenGL/OpenCL) particle simulator 
sitemap: false
hide_last_modified: false
category : pss
image: /assets/img/blog/boidsBox.jpg
accent_image: 
  background: url('/assets/img/blog/boidsBox.jpg') center/cover
  overlay: false
accent_color: '#ccc'
theme_color: '#ccc'
---

# A GPU Boids Simulator

1. this list will be replaced by the table of contents
{:toc}

## Introduction

Creating a real-time physics-based simulation from scratch has been in the back of my mind since 2016. At that time, I was implementing a fluid-structure interaction solver to obtain my master degree in Computational Fluid Dynamics (CFD). Designed for industrial purpose, it was based on a super heavy finite element analysis framework. Imagine inverting sparse matrices with billions of variables at each frame... some of the 3D simulations took literally weeks of computations. When I discovered the rough but almost real-time approaches used for video games and 3D rendering (SPH, FLIP), I was astonished. **I knew that, at some point, I would need to find some time to develop a simulation of this kind.**

Boids came in later. I have always been fascinated by murmurations, this swarm-like collective motion made of thousands of birds, all moving in an unexpected but total synchronisation. Then, a friend told me that a genius named Craig Reynolds had developed a ["boids" model](https://www.red3d.com/cwr/boids/) decades ago. **It was stimulating flocking behavior with 3 simple rules.** I became more and more interested by his model, it sounded amazing to obtain, by simply using math and bits, these beautiful organic patterns. In addition, it was pretty simple to implement and looked like a very nice middle step before coding any fluid simulation. No need to deal with heavy Navier-Stokes equations (differential equations describing fluids motion), but it would still request the development of a point cloud renderer, a physics engine, a GUI window and a real-time application connecting them altogether.

![Murmuration](/assets/img/blog/murmuration.jpg){:.border.lead width="1356" height="668" loading="lazy"}

Starling murmurations. Credit: Menno Schaefer / shutterstock
{:.figcaption}

Besides, if boids simulations are quite common online, I noticed most of them are very limited in scale. There is actually a good reason behind it: the naive implementation of the boids model requests, for each particle, to loop over all the other ones. **This is a very costly asymptotic complexity of O(N<sup>2</sup>)**, N being the number of particles. In other words, a small thousand of particles requires about a million of comparisons at each frame. For such heavy computations, it is hard to do anything consequent with CPUs and their limited number of threads. Fortunately, at the same time, I discovered the huge potential of GPUs and the field of GPGPU (General Programming GPU): basically using GPUs to do cool things beyond rendering a bunch of triangles.

**This is how all the pieces fell into place in my head. Let's do a boids simulator, it looks great and it will prepare the ground for a potential fluid simulation application. But let's go big! Let's do a full GPU particle simulation and display in real time millions of particles moving in a 3D world!**

When starting this project one year ago, I was "a bit" optimistic. I really expected that, with OpenCL, simulating the movement of a million of particles would be a walk in the park. I was obviously wrong and if I stick to the previous metaphor, I ended up going in the mentioned park in the middle of the night with no light and a bunch of stray dogs hunting me. But it was fun and a very formative journey so I decided to write about it. This is the story of my experiments. 

## The first steps

### The application

**Creating an empty C++ desktop application from scratch can be a struggle.** You need to connect all your dependencies: codebases, third-parties, environment scripts, build scripts... Everything must be at the right place with the right connection and the good options. Build it once, fail, fix it, build it twice, fail, fix it and try again. Concerning the build, if you want it to be a bit portable, you need to use [CMake](https://cmake.org/). You also definitely want to add a version control system, [Git](https://git-scm.com/) for example. Besides, it is better to add some formatting tools like [clang-format](https://clang.llvm.org/docs/ClangFormat.html) to keep things clean and pretty. In addition, to have an application with its own cross-platform window, you need to use a specific third-party like [SDL](https://libsdl.org/index.php). If including OpenGL or OpenCL, you need to include specific APIs to connect to your GPU. 

At that time, one of my friends introduced me to [Conan](https://conan.io/) and I happily embraced it. Conan is a wonderful C++ package manager which helps you including third-party libraries already built on remote servers by users or even automatically building them locally. Conan will generate a CMake file pointing to the different generated or retrieved binaries, making the third-parties integration significantly easier.

Overall, this is a lot of things to grasp and understand. It took me literally one weekend to figure this out, choosing the right third-parties, wandering through hollow error messages, stupid mistakes and finally making the whole thing consistent. **All this installation step is definitely not straightforward and from my current perspective, after being done with all of that, I consider it as one of the hardest steps of the whole project.** I think this is probably quite common among C++ developers to struggle with this initialization stage, because we usually work on big projects where someone else took care of setting up everything for us. How privileged we are.

![BlackWindow](/assets/img/blog/blackWindow.png){:.border.lead width="776" height="469" loading="lazy"}

The great black window
{:.figcaption}

### The point cloud renderer

Okay, I have an empty window running on Windows. Great. Now I want to create a box in 3D and be able to move around. **That's another fun bit: implementing a 3D renderer.** First of all, you need a GUI in which you will incorporate a 3D viewport. I choose [Dear ImGui](https://github.com/ocornut/imgui) which is a light-weight, open source, friendly immediate-mode graphics user interface. See it as a fast way to prototype stuff, far away from more complete but heavy retained-mode Guis like Qt. **At each frame, Dear ImGui will regenerate everything, there is no persistent storage of information. It is also directly incorporated in the rendering loop.** All of this makes it easy to prototype and debug. Besides, it can be directly connected to SDL and OpenGL by using [Glad](https://glad.dav1d.de/), another third-party library in the mix! 

When ImGui (let's forget the Dear I am tired) is in place, you can easily create some widgets, connect the mouse and the keyboards with SDL and have something interactive and responsive. Once this is done, the real work on the renderer begins with the implementation of shaders, camera, connection to mouse and keyboard. Using ImGui's OpenGL context, drawing a box is pretty straightforward at that point. I add a bunch of points in it, with random positions and colors, those are my future particles on which I will apply the boids rules. **After days of tweaking stuff, something is finally moving!**

![BoxWithParticles](/assets/img/blog/boxWParts.png){:.border.lead width="349" height="276" loading="lazy"}

Box with particles
{:.figcaption}

## First CPU boids draft

### The physics engine

Particles are finally here! Note that the renderer is only meant to display things and nothing else, it has no clue about physics. **To make the particles behaving like boids, we need a physics engine which computes and updates coordinates of each particle at every frame.** Having a clean and clear codebase with stricly separated submodules is mandatory for me. It helps a lot with debugging and potential futur development. Therefore, interactions between graphics and physics engines are limited as much as possible.

![Triforce](/assets/img/blog/triforce.png){:.border.lead width="349" height="276" loading="lazy"}

Triforce in action
{:.figcaption}

### Boids rules 

**Boids are considered as an emergent behavior, that is, a complex global behavior based on the interaction of a set of simple rules. Similarly to chaos behavior, it has no long term predictability but it differs from it by being predictable on the short term.** This mix of chaotic and predictable movements makes this impression of a life-like phenomena. As presented by [Craig Reynolds in 1986](http://www.red3d.com/cwr/boids/), its boids model is made of three rules :

- **separation** : steer to avoid crowding local flockmates

![boidsSeparation](/assets/img/blog/separation.png){:.border.lead width="349" height="276" loading="lazy"}
Separation. Credit: Craig Reynolds
{:.figcaption}

- **cohesion** : steer to move towards the average position of local flockmates

![boidsCohesion](/assets/img/blog/cohesion.png){:.border.lead width="349" height="276" loading="lazy"}
Cohesion. Credit: Craig Reynolds
{:.figcaption}

- **alignement** : steer towards the average heading of local flockmates

![boidsAlignement](/assets/img/blog/alignement.png){:.border.lead width="349" height="276" loading="lazy"}
Alignement. Credit: Craig Reynolds
{:.figcaption}

As the weighted combination of these three rules can give a variety of different boids behaviors, their weights can be modified in the UI. On top of it, I also implement another optional rule to add an attractive/repulsive target, similar to a boids leader or a predator. 

At time T, the main application calls the physics engine and asks for an update of the particles coordinates. The engine applies boids rules using coordinates and velocity of each particle at T-1 to compute acceleration at T. It then integrates these accelerations by delta time spent between T-1 and T to obtains velocities and finally coordinates at T. During the process, various normalizations and clampings occurs on velocity and coordinates, depending selected boundary conditions and maximum speed. Once the physics engine is done with processing, we still need to load these new coordinates on the GPU to render them, remember, this first draft is CPU-based. Hence, we need to transfer coordinates buffer from CPU to GPU at each frame, this step is not negligible depending the number of particles.

*Physics Engine at work*

For this first CPU implementation of the boids model, I have the chance to be assisted by a friend who is experimented in numerical arts and wants to get a hand on software development. After a bit of tweaking and debugging, we start seeing organic patterns and the expected boids behavior. I spend some time on optimizing the data flow, but the nearest neighbors algorithm remains a brute force loop over all exisiting particles and we quickly reach performance limitations with only 10^3 particles. This is 2-3 orders of magnitude below my ultimate goal, it is time to bring in the GPU artillery!

*1k (CPU - brute force NNS)*

## Hello OpenCL

OpenCL is the Khrono's open framework for heterogeneous programming, giving access to GPUs among other things. It supports task-parallelism, data-parallelism and explicit vectorization accross a various range of platforms. We can compare it to Nvidia CUDA solution when using GPUs. In an OpenCL application, workload is shared between host, usually a CPU running the main program, and devices, one GPU in our case, preserved to run the most computationally demanding processing steps. The host manages the device and prepares the ground for the computation. On the selected device, it implements kernels (shaders) definitions, memory buffers, kernels parameters values and so on. It also decides host-device memory transfers. The devices, when called by the host, do the heavy lifting in a massive parallel fashion using thousands of threads and explicit vector processing. That is a lot of added complexity compared to the original CPU solution and debugging with GPUs is always more challenging, but used carefully, using OpenCL can bring huge performance improvements. I only used OpenCL 1.2 as this is the latest OpenCL standard supported accross the industry.

To have access to GPU power through OpenCL, we need to add extra steps to our processing. First, we use Conan to retrieve the OpenCL ICD (installer client loader). The ICD is a header file allowing us to compile with OpenCL features but also, at run time, to select available OpenCL device, build its device-specific binaries and run the kernels. Now, to keep everything clean and simple, we need to add an extra OpenCL layer in the physics engine because we don't want to have direct OpenCL calls in our boids submodule. Physics is one thing, host-device communication another. Besides, OpenCL calls are generic and will probably be used by other physics submodules at some point. Hence, the arrival of an OpenCL wrapper, doing the interface between my physics engine and the OpenCL API.

*Connecting to allmighty GPU*

Apart from increasing performance by massively using data-parallelism, OpenCL also removes costly CPU-GPU transfers. With CPU boids implementation, we had to transfer all new coordinates to GPU renderer at every frame, remember? When you have a few hundreds of entities, no big deal, but when you start playing with hundreds of particles, it means transferring a few mega octets between GPU and CPU at every frame. It takes time and impacts performance. The good news is interoperation is supported between OpenGL and OpenCL. Basically, OpenGL and OpenCL can share specified GPU memory buffers and therefore ensure zero copy between computing and rendering steps. When starting its processing, the physics engine takes full control of the coordinates buffers and passes it back to the renderer when done. Beautiful isn't it? It requires some adjustments, though. You need to ensure that OpenCL is using OpenGL context and that both frameworks are using the same device, that means I have to add some logic in the OpenCL wrapper to detect which device (IGPU or discrete GPU?) is used to render the application and use it for the physics processing. So, make sure that you launch the application with your most powerful GPU as it will be used for both rendering and processing.

After loosing some evenings and nights in debugging mode, it is finally behaving the same way as with CPU, nice. Obviously, my next move is to increase as much as I can the number of particles. After a bit of adjustements at kernel level, I finally cap at 30k particles for 30-40 FPS. Wait, what? 30k? Where is my million?

*30k (GPU - brute force NNS)*

## Advanced mode unlocked

Using OpenCL does increase performance by more than an order of magnitude, from 1k to 30k, but this is not enough. To go further, it is time to focus on a well known problem in the N-body simulation world, the nearest neighbor search bottleneck. For this type of simulation, NNS algorithms are critical. You have N particles and each particle is under the influence of its neighbors from a given vicinity, you need to compute this effect but how do you find those neighbors among N? Brute force algorithm loops over all N particles to check the distance between the two particles, the one considered and its potential neighbor, giving us an asymptotic complexity of O(N^2). Using thousands of GPU threads doesn't fix the problem as every thread will still have to run this potential huge loop, hence the cap at 30k. Luckily, there is a vast litterature on the NNS subject with more advanced solutions. After reading a small dozen of articles, I decide to go for Simon Green's simple but very efficient proposition based on a combination of spatial partioning and sorting.

The idea is to split the 3D world in cubic cells and tag each particle with the cell ID where it is found. Once every particle has been tagged, it is then possible to focus the search only in the neighboring cells. To get rid of the O(N^2) time complexity without adding complex mapping structures between cells and particles, you then need to sort the particle coordinates, velocity and acceleration buffers by using their cell ID. Indeed doing so allows us, for each particle, to loop among a small and specific subset of neighboring particles of the global buffers while computing the boids rules.

*NNS cell grid*

*Radix sort*

Adding the spatial partioning step is quite straightforward with a box for 3D world. You have a direct connection between particle position and cell index that can be used as a unique ID. In order to debug it, the cell grid is sent to the renderer and displayed with the particles. Pretty neat visual effect in bonus! The sorting step is more complicated, there are many sequential sorting algorithms out there but few parallel ones, roughly radix sort and bitonic sort. Both seem to be super fast and comprehensible. Apart from performance, I am mostly concerned by how straightforward it can be implemented. After some testing, I decide to go with Radix sort as there is one open-source OpenCL implementation on github and it seems faster than bitonic sort. I take some time to simplify the excellent implementation made by ?,  adapt it and plug it to my OpenCL wrapper and boids model. Behind those simple words hide a few weeks of hard work. Implementing this GPU spatial partioning + radix + boids pipeline is probably the hardest part of the whole project, quite a lot of ~~pain~~ fun involved. Many things can go at the different steps of the process and I need to debug it based on some emergent behavior hardly predictable. I even get stuck with a terrible bug for weeks without noticing it, having nothing to compare to and naively assuming that with a lot of particles involved, things can be a bit messy. Only after unloading particle buffers at each step for something else, I realize that everything is broken because I don't refresh an intermediary buffer of the radix sort at the beginning of each frame.

*before, after the bug*

So here is the whole pipeline, at each time frame, I start with tagging all particles with their cell IDs. Once done, I sort with the Radix sort the cell ID buffer and use it to permutate in the same order coordinates, velocity and acceleration buffers. Still with the cell ID buffer, I can also scan it in order to know how many particles are in each cell and generate the cell count buffer, providing the start and end indices of particles buffers for particles contained in each cell, this information is very important for the next step, the boids model. At this stage, I apply the boids rules on every particle without looping on the whole particle buffer for the NNS, only selecting very specific parts of the global buffer where I am certain to find neighbors using the cell count buffer. Once done, the rest is similar to the CPU prototype, integration of the acceleration, various clampings and normalizations to obtain new coordinates in the 3D world. Absolutely, everything explained above is done on GPU, no CPU-GPU memory transfer at computing or rendering steps except for the kernel and shader parameters. Pure power.

*Advance GPU physics engine pipeline   Spatial partioning + radix + boids*

The main drawback of the spatial partioning is that it can still contain hidden asymptotic complexity of O(N^2). Indeed, if every particles are in the same cell, you are back to square one, looping on every one of them for the NNS. With the right cell resolution, this can be prevented in most cases, but not all of them. The O(N^2) instant lagg appears when an attractive target is added to the system and all particles converge to it, in these cases the drop of FPS is sharp. One easy solution is limiting the number of particles visited in each cell at NNS step, something like: "visit the 15000 first neighbors in this cell and forget the remaining ones". This is not perfect as it can generate noticable visual artifacts in boids behavior and is not a pure boids model anymore.

At that point, I spent a lot of time working on the optimization of the kernels, the theorical O(N^2) barrier was gone so I had room to make things faster. I improved the critical boids kernels bit by bit, streamlined the process, improved the data flow, removed unecessary steps, tweaked the parameters. I also went deep in the algorithms trying to implement the use of GPU local memory in the kernels. I won't go too much into details for this one as it would mean explaining the complete memory framework of OpenCL but in short it didn't work well. I was a bit surprised by the results because usually using local memory over global memory means significant performance improvements. However after looking at different other GPU N-body simulation implementations, it appears that my lack of success with local memory was quite common for this type of simulation. The performance with global memory was higher thanks to a hidden phenomenon permitted by the sorting of the particles, coalesced memory access. Basically the memory transactions are drastically improved in this case because neighboring threads compute neighboring sorted particles and use the same data for the computation, reducing costly memory transaction with global memory. Therefore, I stuck with the global memory model. After weeks of work, I finally reached 260k particles running in real time at around 60 FPS with my Nvidia GTX1060. Jumping from 1k to 260k, not bad. I could definitely go higher, things are far from being perfect but I am happy with the results and it is already a lot of particles, the box has been expanded 3 times already and is still cramped by all the boids. The 1 million particles objective can definitely wait.

*260k (GPU - radix sort - spatial partioning NNS)*

## Graphics update

So here I am with my 260k particles running smoothly on my machine, happy. One thing which is still bugging me, though, is the visual aspect of the simulation. I haven't spent much time working on it as the original plan was to work on GPGPU performance and advanced computation. Nevertheless, it is time for a small graphics enhancement to make things agreable to watch, right now it is pretty ugly to be honest. Fortunately, I am pleasantly surprised by the numbers of hardcoded features available in the old OpenGL state machine. In a few lines, I am able to add some antialising, point smoothing and additive blending, I also put some colors in the mix. Contrasts are striking. 

* Image blending and other improvements*

I also slightly refine the vertex shaders to have a point size changing in function of the distance with the camera, in order to have an impression of depth. Doing so, I notice that sometimes some particles in the background are drawn on top of front ones, not good. To prevent it, I need to compute the particle-camera distance for every particle in the physics engine and sort all the particle buffers again in function of this distance. Performance are not even impacted by the second sorting, proving how fast this radix sorting is!

*Distance-dependant particle size*

*Final 260k with enhanced graphics*

## Perlin Target

There is still one last thing to achieve, I want to implement a moving target in the 3D world. The current one is static in the center of the box. First, I think about allowing the user to control it with the mouse but this control is already used to rotate and translate the camera, usability will get tricky. Thus, I prefer to implement a random automatic trajectory and let the user admire it. Perlin noise is the perfect solution for this problem, invented by Ken Perlin in 1983, it gives a more lifelike, natural randomness than classical random value generator. Based on a 3D grid with pseudo random gradient values set at its vertices, it generates random values in 3D space with a sense of spatial coherency. By arranging a unique Perlin noise generator with different gradients for each spherical coordinates, I am able to implement a target moving smoothly in the box around its center, its speed being adjusted in function of particle speed. I am pleased with the results, it gives nice effects with the impression of flocks being after some food or fleeing a predator.

## Final thoughts

I think it is time for a conclusion. All in all, it was an amazing experience. When I think of the first weekend almost one year ago, struggling with an empty black window, I am quite amazed by how far I managed to go with a bit of dedication and skills. I have definitely learned a ton about GPGPU, software development but also myself. Working alone in the evening after having spent the day on your daily developer's tasks demands some determination. Nobody will go after you if the project is not progessing, you are on your own and moving away from the screen is tempting. Of course there is still a lot of things to improve or implement (more optimization, more advanced boids model...), but there will always be and I think it is time to move on to something else. So what's next? Well, good question, maybe a parallel sorting algorithms visualizer or a Vulkan renderer from scratch. Of course, I haven't forget about the fluid simulation either! Will see how it goes. If you read this line, it means you have been patient enough to go through all my gibberish, thanks a lot for your interest and don't hesitate to contact if you have any question!
